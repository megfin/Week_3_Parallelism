{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data parallelism: Exercise\n",
    "\n",
    "For this exercise we will be build upon last week's vanilla gradient descent example. Included in the next codebox are functions to perform feedforward and backprop on a single minibatch. The computeMinibatchGradientsTuple() function is the same as the computeMinibatchGradients() function, but its inputs in a single tuple will make using Python's ThreadPool easier later on.\n",
    "\n",
    "You donâ€™t need to do modify this first block of code. \n",
    "\n",
    "If you do not have scikit-learn then you can get it here: https://scikit-learn.org/stable/install.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "# In order to run this in class, we're going to reduce the dataset by a factor of 5\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "X = X[::5]\n",
    "y = y.astype(int)[::5]\n",
    "X, X_test, y, y_test = train_test_split(X, y)\n",
    "\n",
    "# Here we specify the size of our neural network.\n",
    "# We are mapping from 784 to 10 with 256 hiden layer nodes.\n",
    "\n",
    "m = len(X)\n",
    "n_0 = 784\n",
    "n_1 = 256\n",
    "N = 10\n",
    "\n",
    "\n",
    "# Function to convert categorical labels into one-hot matrix.\n",
    "def convert_to_one_hot(y, n_classes):\n",
    "    T = np.zeros((y.shape[0], n_classes))\n",
    "    for t, yy in zip(T, y):\n",
    "        t[yy] = 1\n",
    "    return T\n",
    "\n",
    "\n",
    "# Convert the data to one hot notation\n",
    "one_hot_y_actual = convert_to_one_hot(y, N)\n",
    "one_hot_y_test = convert_to_one_hot(y_test, N)\n",
    "\n",
    "\n",
    "# Sigmoid function (activation)\n",
    "def sigmoid(a):\n",
    "    return 1. / (1 + np.exp(-a))\n",
    "\n",
    "\n",
    "# Softmax function (final layer for classification)\n",
    "def softmax(A):\n",
    "    numerator = np.exp(A)\n",
    "    denominator = numerator.sum(axis=1)\n",
    "    return numerator / denominator[:, np.newaxis]\n",
    "\n",
    "\n",
    "# Categorical cross-entropy\n",
    "def L(T, S, W1, W2, alpha_1=1e-2, alpha_2=1e-5):\n",
    "    return -1. / len(T) * np.sum(T * np.log(S)) + np.sum(0.5 * alpha_1 * W1 ** 2) + np.sum(0.5 * alpha_2 * W2 ** 2)\n",
    "\n",
    "\n",
    "# Run the neural network forward, given some weights and biases\n",
    "def feedforward(X, W1, W2, b1, b2):\n",
    "    # Feedforward\n",
    "    A1 = X @ W1 + b1\n",
    "    Z1 = sigmoid(A1)\n",
    "    A2 = Z1 @ W2 + b2\n",
    "    y_pred = softmax(A2)\n",
    "    return y_pred, Z1\n",
    "\n",
    "\n",
    "# Compute the neural network gradients using backpropagation\n",
    "def backpropogate(y_pred, Z1, X, y_obs, alpha_1=1e-2, alpha_2=1e-5):\n",
    "    # Backpropogate\n",
    "    delta_2 = (1. / len(y_pred)) * (y_pred - y_obs)\n",
    "    grad_W2 = Z1.T @ delta_2 + alpha_2 * W2\n",
    "    grad_b2 = delta_2.sum(axis=0)\n",
    "\n",
    "    delta_1 = delta_2 @ W2.T * Z1 * (1 - Z1)\n",
    "    grad_W1 = X.T @ delta_1 + alpha_1 * W1\n",
    "    grad_b1 = delta_1.sum(axis=0)\n",
    "    return grad_W1, grad_W2, grad_b1, grad_b2\n",
    "\n",
    "\n",
    "def mini_batch(x_sample, y_sample, start_batch_size):\n",
    "    \"\"\"\n",
    "    Takes a copy of x_sample and y_sample and returns mini batch matrices of both and number of batches\n",
    "    \"\"\"\n",
    "\n",
    "    # Batches must divide evenly into total number of samples for numpy arrays to be happy.\n",
    "    # Gets number of bathes by finding next smallest number that evenly divides\n",
    "    num_batches = start_batch_size\n",
    "    while len(x_sample) % num_batches != 0:\n",
    "        num_batches -= 1\n",
    "\n",
    "    # randomly shuffle indices\n",
    "    np.random.seed(42)\n",
    "    random_indices = np.random.choice(range(len(x_sample)), len(x_sample), replace=False)\n",
    "\n",
    "    # instantiate lists to hold batches\n",
    "    x_list = [[] for i in range(num_batches)]\n",
    "    y_list = [[] for i in range(num_batches)]\n",
    "\n",
    "    # populate batches matrix with random mini batch indices\n",
    "    for i in range(len(x_sample)):\n",
    "\n",
    "        x_list[i // 105].append(x_sample[random_indices[i]])\n",
    "        y_list[i // 105].append(y_sample[random_indices[i]])\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    x_batch = np.array(x_list)\n",
    "    y_batch = np.array(y_list)\n",
    "\n",
    "    return x_batch, y_batch, num_batches, num_batches\n",
    "\n",
    "\n",
    "#computes the gradients of a single minibatch\n",
    "def computeMinibatchGradients(W1, W2, b1, b2, x_batch, y_batch):\n",
    "    y_pred, Z1 = feedforward(x_batch, W1, W2, b1, b2)\n",
    "    \"\"\"\n",
    "    These are your gradients with respect to weight matrices W1 and W2 \n",
    "    as well as your biases b1 and b2\n",
    "    \"\"\"\n",
    "    grad_W1, grad_W2, grad_b1, grad_b2 = backpropogate(y_pred, Z1, x_batch, y_batch)\n",
    "    \n",
    "    return grad_W1, grad_W2, grad_b1, grad_b2\n",
    "\n",
    "#computes the gradients of a single minibatch\n",
    "def computeMinibatchGradientsTuple(inputTuple):\n",
    "    W1, W2, b1, b2, x_batch, y_batch = inputTuple\n",
    "    y_pred, Z1 = feedforward(x_batch, W1, W2, b1, b2)\n",
    "    \"\"\"\n",
    "    These are your gradients with respect to weight matrices W1 and W2 \n",
    "    as well as your biases b1 and b2\n",
    "    \"\"\"\n",
    "    grad_W1, grad_W2, grad_b1, grad_b2 = backpropogate(y_pred, Z1, x_batch, y_batch)\n",
    "    \n",
    "    return grad_W1, grad_W2, grad_b1, grad_b2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Gradient Descent\n",
    "\n",
    "This next codebox should look familiar; it performs vanilla gradient descent. You don't need to change this codebox, either. Run this, and notice that it now also prints out the time taken to evaluate each epoch. We'll use these times to evaluate how much of a speedup data parallelism will give us in a simple multithreading environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 2.166623 Accuracy 0.633143 time taken 573 ms\n",
      "Epoch 10 Loss 0.777833 Accuracy 0.894571 time taken 603 ms\n",
      "Epoch 20 Loss 0.462886 Accuracy 0.915143 time taken 643 ms\n",
      "Epoch 30 Loss 0.340971 Accuracy 0.921143 time taken 604 ms\n",
      "Epoch 40 Loss 0.275558 Accuracy 0.924000 time taken 602 ms\n",
      "Epoch 50 Loss 0.234138 Accuracy 0.924857 time taken 615 ms\n",
      "Epoch 60 Loss 0.205114 Accuracy 0.926000 time taken 650 ms\n",
      "Epoch 70 Loss 0.183592 Accuracy 0.926000 time taken 631 ms\n",
      "Epoch 80 Loss 0.166841 Accuracy 0.927429 time taken 622 ms\n",
      "Epoch 90 Loss 0.153473 Accuracy 0.927714 time taken 626 ms\n",
      "Epoch 100 Loss 0.142438 Accuracy 0.928857 time taken 633 ms\n",
      "Epoch 110 Loss 0.133112 Accuracy 0.928857 time taken 640 ms\n",
      "Epoch 120 Loss 0.125195 Accuracy 0.930000 time taken 607 ms\n",
      "Epoch 130 Loss 0.118398 Accuracy 0.930571 time taken 612 ms\n",
      "Epoch 140 Loss 0.112438 Accuracy 0.930857 time taken 567 ms\n",
      "Epoch 150 Loss 0.107235 Accuracy 0.931429 time taken 591 ms\n",
      "Epoch 160 Loss 0.102699 Accuracy 0.930286 time taken 611 ms\n",
      "Epoch 170 Loss 0.098641 Accuracy 0.930000 time taken 613 ms\n",
      "Epoch 180 Loss 0.094974 Accuracy 0.931429 time taken 599 ms\n",
      "Epoch 190 Loss 0.091654 Accuracy 0.931143 time taken 591 ms\n",
      "Epoch 200 Loss 0.088618 Accuracy 0.930286 time taken 593 ms\n",
      "Epoch 210 Loss 0.085856 Accuracy 0.930857 time taken 651 ms\n",
      "Epoch 220 Loss 0.083397 Accuracy 0.931143 time taken 609 ms\n",
      "Epoch 230 Loss 0.081139 Accuracy 0.930857 time taken 581 ms\n",
      "Epoch 240 Loss 0.078988 Accuracy 0.930571 time taken 579 ms\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Vanilla Gradient Descent\n",
    "\"\"\"\n",
    "\n",
    "# Hyper Parameters\n",
    "eta = 1e-3\n",
    "initial_batch_size = 104\n",
    "epochs = 250\n",
    "\n",
    "# Initialize random parameter matrices\n",
    "np.random.seed(42)\n",
    "W1 = 0.001 * np.random.randn(n_0, n_1)\n",
    "W2 = 0.001 * np.random.randn(n_1, N)\n",
    "\n",
    "b1 = 0.1 * np.random.randn(1, n_1)\n",
    "b2 = 0.1 * np.random.randn(1, N)\n",
    "\n",
    "# data for analysis\n",
    "vanilla_loss = []\n",
    "\n",
    "\n",
    "# Perform gradient descent\n",
    "for i in range(epochs):\n",
    "    epochStartTime = time.time()\n",
    "    \n",
    "    # generate mini batches\n",
    "    x_batches, y_batches, num_batches, actual_batch_size = mini_batch(X, one_hot_y_actual, initial_batch_size)\n",
    "\n",
    "    # perform gradient descent on mini batches\n",
    "    for j in range(0, num_batches):\n",
    "        \n",
    "        grad_W1, grad_W2, grad_b1, grad_b2 = computeMinibatchGradients(W1, W2, b1, b2, x_batches[j], y_batches[j])\n",
    "        '''\n",
    "        use the gradients to update weights and biases\n",
    "        '''\n",
    "        W1 -= eta * grad_W1\n",
    "        W2 -= eta * grad_W2\n",
    "        b1 -= eta * grad_b1\n",
    "        b2 -= eta * grad_b2\n",
    "\n",
    "\n",
    "    # calc loss at end of each epoch\n",
    "    y_entire_pred, Z1 = feedforward(X, W1, W2, b1, b2)\n",
    "    vanilla_loss.append(L(one_hot_y_actual, y_entire_pred, W1, W2))\n",
    "    \n",
    "    #find the time taken to compute the epoch\n",
    "    epochTimeTaken = (time.time() - epochStartTime)*1000\n",
    "\n",
    "    # Print some summary statistics every ten iterations\n",
    "    if i % 10 == 0:\n",
    "        y_pred_test, Z1_test = feedforward(X_test, W1, W2, b1, b2)\n",
    "        acc = sum(y_test == np.argmax(y_pred_test, axis=1)) / len(y_test)\n",
    "        y_entire_pred, Z1 = feedforward(X, W1, W2, b1, b2)\n",
    "        print(\"Epoch %d Loss %f Accuracy %f time taken %d ms\" % (i, L(one_hot_y_actual, y_entire_pred, W1, W2), acc, epochTimeTaken))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updating Vanilla Gradient Descent with Data Parallelism\n",
    "\n",
    "Now that we have some baseline timings, we're going to be updating this example to employ data parallelism. The Ben-Nun et.al. paper mainly focuses on parallelism in a distributed computing environment, but using a library like MPI for distributed parallelism would be well outside the scope of these assignments, so we're going to using Python's multiprocessing package to perform data parallelism with a ThreadPool\n",
    "\n",
    "First, read the documentation on python's Pool class, located here:\n",
    "https://docs.python.org/2/library/multiprocessing.html#module-multiprocessing\n",
    "\n",
    "We're going to be using the Pool's faster (and less documented) cousin, the ThreadPool. The Pool and ThreadPool have the same interface, but while Pool uses a single thread, trading it between the pool's workers, the ThreadPool actually spins up multiple instances of the Python interpreter in different threads to perform true parallel computation.\n",
    "\n",
    "The next codeblock uses the ThreadPool's map function to give each process a different minibatch in parallel. \n",
    "\n",
    "\n",
    "1.\n",
    "On line 60, use the ThreadPool's map function to parallelize the gradient calculation for each of the parallel batches.\n",
    "\n",
    "2.\n",
    "We will need to average the gradients returned from each parallel batch in order to perform gradient descent, but the thread pool returns a list of the list of each batch's gradients. To make averaging the gradients easier, line 58 uses the zip function to make a new list such that the first element in the list contains all the W1 gradients, the second element contains all the W2 gradients, etc. On lines 59-62, use the np.mean function to average all W1, W2, b1, and b2 gradients, and use those averages to update W1, W2, b1, and b2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 2.283451 Accuracy 0.100571 time taken 760 ms\n",
      "Epoch 10 Loss 1.762700 Accuracy 0.672000 time taken 814 ms\n",
      "Epoch 20 Loss 1.307618 Accuracy 0.776286 time taken 811 ms\n",
      "Epoch 30 Loss 1.012542 Accuracy 0.860571 time taken 804 ms\n",
      "Epoch 40 Loss 0.819719 Accuracy 0.890857 time taken 804 ms\n",
      "Epoch 50 Loss 0.688920 Accuracy 0.902286 time taken 808 ms\n",
      "Epoch 60 Loss 0.595744 Accuracy 0.906000 time taken 814 ms\n",
      "Epoch 70 Loss 0.526461 Accuracy 0.911143 time taken 828 ms\n",
      "Epoch 80 Loss 0.473078 Accuracy 0.915714 time taken 827 ms\n",
      "Epoch 90 Loss 0.430669 Accuracy 0.917429 time taken 866 ms\n",
      "Epoch 100 Loss 0.396118 Accuracy 0.919429 time taken 825 ms\n",
      "Epoch 110 Loss 0.367435 Accuracy 0.920571 time taken 814 ms\n",
      "Epoch 120 Loss 0.343198 Accuracy 0.921429 time taken 814 ms\n",
      "Epoch 130 Loss 0.322427 Accuracy 0.921429 time taken 817 ms\n",
      "Epoch 140 Loss 0.304414 Accuracy 0.921714 time taken 837 ms\n",
      "Epoch 150 Loss 0.288614 Accuracy 0.922286 time taken 804 ms\n",
      "Epoch 160 Loss 0.274631 Accuracy 0.922857 time taken 819 ms\n",
      "Epoch 170 Loss 0.262162 Accuracy 0.923714 time taken 868 ms\n",
      "Epoch 180 Loss 0.250956 Accuracy 0.924286 time taken 828 ms\n",
      "Epoch 190 Loss 0.240823 Accuracy 0.925143 time taken 832 ms\n",
      "Epoch 200 Loss 0.231622 Accuracy 0.925714 time taken 834 ms\n",
      "Epoch 210 Loss 0.223211 Accuracy 0.926857 time taken 825 ms\n",
      "Epoch 220 Loss 0.215488 Accuracy 0.926857 time taken 809 ms\n",
      "Epoch 230 Loss 0.208372 Accuracy 0.926857 time taken 855 ms\n",
      "Epoch 240 Loss 0.201796 Accuracy 0.926857 time taken 861 ms\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Vanilla Gradient Descent with Data Parallelism\n",
    "\"\"\"\n",
    "\n",
    "#import the ThreadPool\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "import os\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "\n",
    "# Hyper Parameters\n",
    "eta = 1e-3\n",
    "initial_batch_size = 104\n",
    "epochs = 250\n",
    "\n",
    "#add additional hyperparameters related to the data parallelism\n",
    "threads_in_pool = 4\n",
    "parallel_batches = 4\n",
    "\n",
    "#create the thread pool\n",
    "pool = ThreadPool(processes=threads_in_pool) \n",
    "\n",
    "\n",
    "# Initialize random parameter matrices\n",
    "np.random.seed(42)\n",
    "W1 = 0.001 * np.random.randn(n_0, n_1)\n",
    "W2 = 0.001 * np.random.randn(n_1, N)\n",
    "\n",
    "b1 = 0.1 * np.random.randn(1, n_1)\n",
    "b2 = 0.1 * np.random.randn(1, N)\n",
    "\n",
    "# data for analysis\n",
    "vanilla_loss = []\n",
    "\n",
    "# Perform gradient descent\n",
    "for i in range(epochs):\n",
    "    epochStartTime = time.time()\n",
    "    \n",
    "    # generate mini batches\n",
    "    x_batches, y_batches, num_batches, actual_batch_size = mini_batch(X, one_hot_y_actual, initial_batch_size)\n",
    "    \n",
    "    \n",
    "    # perform gradient descent on mini batches\n",
    "    for j in range(0, num_batches, parallel_batches):\n",
    "        \n",
    "        #create the list of inputs for the pool threads\n",
    "        #this might look weird, but by creating a list of tuples, the input data can be easily given to\n",
    "        #each worker thread in the ThreadPool\n",
    "        minibatchGradientInputLists = []\n",
    "        for k in range(parallel_batches):\n",
    "            minibatchGradientInputLists.append((W1, W2, b1, b2, x_batches[j+k], y_batches[j+k]))\n",
    "        \n",
    "        #TODO: use the ThreadPool's map function to compute minibatch gradients in parallel.\n",
    "        gradientOutputs = pool.map(computeMinibatchGradientsTuple, minibatchGradientInputLists)\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        use the gradients to update weights and biases\n",
    "        '''\n",
    "        gradients = list(zip(*gradientOutputs))\n",
    "        W1 -= eta * np.mean(gradients[0], axis = 0) #TODO: average (np.mean()) the W1 gradients we put into a list above.\n",
    "        W2 -= eta * np.mean(gradients[1], axis = 0) #TODO: average (np.mean()) the W2 gradients we put into a list above.\n",
    "        b1 -= eta * np.mean(gradients[2], axis = 0) #TODO: average (np.mean()) the b1 gradients we put into a list above.\n",
    "        b2 -= eta * np.mean(gradients[3], axis = 0) #TODO: average (np.mean()) the b2 gradients we put into a list above.\n",
    "\n",
    "    # calc loss at end of each epoch\n",
    "    y_entire_pred, Z1 = feedforward(X, W1, W2, b1, b2)\n",
    "    vanilla_loss.append(L(one_hot_y_actual, y_entire_pred, W1, W2))\n",
    "    \n",
    "    #find the time taken to compute the epoch\n",
    "    epochTimeTaken = (time.time() - epochStartTime) * 1000\n",
    "\n",
    "    # Print some summary statistics every ten iterations\n",
    "    if i % 10 == 0:\n",
    "        y_pred_test, Z1_test = feedforward(X_test, W1, W2, b1, b2)\n",
    "        acc = sum(y_test == np.argmax(y_pred_test, axis=1)) / len(y_test)\n",
    "        y_entire_pred, Z1 = feedforward(X, W1, W2, b1, b2)\n",
    "        print(\"Epoch %d Loss %f Accuracy %f time taken %d ms\" % (i, L(one_hot_y_actual, y_entire_pred, W1, W2), acc, epochTimeTaken))\n",
    "\n",
    "#kill the pool so it doesn't hang around without getting garbage collected\n",
    "pool.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance assessment and questions\n",
    "\n",
    "Now that your data parallel implementation is finished, play around with the threads_in_pool and parallel_batches hyperparameters, and answer the following questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How does the speed of the data parallel implementation compare to the non-parallelized version.\n",
    "    \n",
    "    \n",
    "Answer: Using 4 threads and 4 parallel batches doesn't seem to improve the speed of the algorithm. It is possible that   tweaking the these parameters will improve the speed. \n",
    "\n",
    "\n",
    "2. Adjusting the threads_in_pool and parallel_batches hyperparameters, where do you see the most improvement in speed? When does increasing these hyperparameters stop making the computation faster?\n",
    "\n",
    "\n",
    "Answer: For this particular example, most combinations don't do anything to speed up the process. However, in theory, it would make sense that running all data records concurrently could be the fastest.\n",
    "\n",
    "3. Section 3 of the paper discusses Generalization in the context of statistical accuracy. How does the generalization issue relate to the parallel_batches hyperparameter?\n",
    "\n",
    "\n",
    "Answer: The generalization issue states that your minibatches shouldn't be too large or too small. Too small of batches won't harness the concurrency in evaluating the loss function and too large will result in a decay in quality beyond a certain point. Relating this to the parallel_batches hyperparameter, if we have too small of an initial minibatch, setting the parallel_batches means we will be running our threads with very few examples. If the minibatch is too small, the parallel_batches will have too many examples to yield quality results. \n",
    "\n",
    "\n",
    "4. Using a library like mpi4py, we could take the local, thread-parallel approach and do it in a true distributed environment. If the computeMinibatchGradients function was being run on different processors in a distributed system, what data would you have to send to the processors for each minibatch? What information would these distributed processors need to send back?\n",
    "\n",
    "\n",
    "Answer: Each processor would need to send back the gradient for each minibatch that way we know how much to update the weights.\n",
    "\n",
    "\n",
    "4. As we discussed in class on Tuesday, model parallelism involves splitting up a network between processors such that different portions of the same layer might be computed on different processors. Knowing that the example network is comprised of two full-connected layers, what changes would you have to make to the code to be able to employ model parallelism. (Note, actually doing this would be an enormous amount of work, but think critically about which parts of the network would need to be rewritten to achieve model parallelism.) \n",
    "\n",
    "\n",
    "Answer: We would need to specify which layer is on what processor as well as which nodes from that layer are going to each processor. We would also need more communication between processors to compute the gradient and update the weights. If we drew out a fully-connected NN, model parallelism would be like horizontally cross sectioning off parts and assigning them to processors.\n",
    "\n",
    "\n",
    "5. Pipeline parallelism involves splitting up a network between processors such that each processor is responsible for one or more contiguous operators. How might you change the example to perform pipeline-parallelism? Would this be easier to implement than model parallelism, or harder?\n",
    "\n",
    "\n",
    "Answer: To implement pipeline parallelism we can think of assigning individual layers of a fully-connected NN to processors. Once the layers are assigned to processors, we need to assign order. Once the first layer is done, we can pass that information to the next processor and start work on the first processor again. This method will have latency or some waiting time, but will be easier to implement than model parallelism.\n",
    "\n",
    "\n",
    "6. If a pipeline-parallel network such as the one from the previous question was implemented, how would data quantization help improve performance in a distributed environment?\n",
    "\n",
    "\n",
    "Answer: Quantization will help in a distributed computing environment because we are mapping continuous values to bins which will help reduce memory constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
